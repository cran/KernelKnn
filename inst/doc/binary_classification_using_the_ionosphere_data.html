<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Lampros Mouselimis" />

<meta name="date" content="2017-02-10" />

<title>binary classification using the ionosphere data</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">binary classification using the ionosphere data</h1>
<h4 class="author"><em>Lampros Mouselimis</em></h4>
<h4 class="date"><em>2017-02-10</em></h4>



<p>The following examples illustrate the functionality of the KernelKnn package for <strong>classification</strong> tasks. I’ll make use of the <em>ionosphere</em> data set,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(ionosphere, <span class="dt">package =</span> <span class="st">'KernelKnn'</span>)

<span class="kw">apply</span>(ionosphere, <span class="dv">2</span>, function(x) <span class="kw">length</span>(<span class="kw">unique</span>(x)))</code></pre></div>
<pre><code>##    V1    V2    V3    V4    V5    V6    V7    V8    V9   V10   V11   V12 
##     2     1   219   269   204   259   231   260   244   267   246   269 
##   V13   V14   V15   V16   V17   V18   V19   V20   V21   V22   V23   V24 
##   238   266   234   270   254   280   254   266   248   265   248   264 
##   V25   V26   V27   V28   V29   V30   V31   V32   V33   V34 class 
##   256   273   256   281   244   266   243   263   245   263     2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># the second column will be removed as it has a single unique value</span>

ionosphere =<span class="st"> </span>ionosphere[, -<span class="dv">2</span>]</code></pre></div>
<p><br></p>
<p>When using an algorithm where the ouput depends on distance calculation (as is the case in k-nearest-neighbors) it is recommended to first scale the data,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># recommended is to scale the data</span>

X =<span class="st"> </span><span class="kw">scale</span>(ionosphere[, -<span class="kw">ncol</span>(ionosphere)])
y =<span class="st"> </span>ionosphere[, <span class="kw">ncol</span>(ionosphere)]</code></pre></div>
<p><br></p>
<p><strong>important note</strong> : In classification, both functions <em>KernelKnn</em> and <em>KernelKnnCV</em> accept a numeric vector as a response variable (here y) and the unique values of the labels should begin from 1. This is important otherwise the internal functions do not work. Furthermore, both functions (by default) return predictions in form of probabilities, which can be converted to labels by using either a threshold (if binary classification) or the maximum value of each column (if multiclass classification).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># labels should be numeric and begin from 1:Inf</span>

y =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>:<span class="kw">length</span>(<span class="kw">unique</span>(y)))[ <span class="kw">match</span>(ionosphere$class, <span class="kw">sort</span>(<span class="kw">unique</span>(ionosphere$class))) ]

<span class="co"># random split of data in train and test</span>

spl_train =<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">length</span>(y), <span class="kw">round</span>(<span class="kw">length</span>(y) *<span class="st"> </span><span class="fl">0.75</span>))
spl_test =<span class="st"> </span><span class="kw">setdiff</span>(<span class="dv">1</span>:<span class="kw">length</span>(y), spl_train)
<span class="kw">str</span>(spl_train)</code></pre></div>
<pre><code>##  int [1:263] 38 203 136 316 2 108 121 226 171 320 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(spl_test)</code></pre></div>
<pre><code>##  int [1:88] 1 6 7 10 15 16 18 23 24 26 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># evaluation metric</span>

acc =<span class="st"> </span>function (y_true, preds) {
  
  out =<span class="st"> </span><span class="kw">table</span>(y_true, <span class="kw">max.col</span>(preds, <span class="dt">ties.method =</span> <span class="st">&quot;random&quot;</span>))
  
  acc =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">diag</span>(out))/<span class="kw">sum</span>(out)
  
  acc
}</code></pre></div>
<div id="the-kernelknn-function" class="section level2">
<h2>The KernelKnn function</h2>
<p>The KernelKnn function takes a number of arguments. To read details for each one of the arguments type ?KernelKnn::KernelKnn in the console.</p>
<p>A simple k-nearest-neighbors can be run with weights_function = NULL and the parameter ‘regression’ should be set to FALSE. In classification the <em>Levels</em> parameter takes the unique values of the response variable,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(KernelKnn)

preds_TEST =<span class="st"> </span><span class="kw">KernelKnn</span>(X[spl_train, ], <span class="dt">TEST_data =</span> X[spl_test, ], y[spl_train], <span class="dt">k =</span> <span class="dv">5</span> , 
                       
                       <span class="dt">method =</span> <span class="st">'euclidean'</span>, <span class="dt">weights_function =</span> <span class="ot">NULL</span>, <span class="dt">regression =</span> F,
                       
                       <span class="dt">Levels =</span> <span class="kw">unique</span>(y))
<span class="kw">head</span>(preds_TEST)</code></pre></div>
<pre><code>##      class_1 class_2
## [1,]     0.2     0.8
## [2,]     0.8     0.2
## [3,]     0.0     1.0
## [4,]     0.8     0.2
## [5,]     0.0     1.0
## [6,]     0.6     0.4</code></pre>
<p><br> There are two ways to use a kernel in the KernelKnn function. The <strong>first option</strong> is to choose one of the existing kernels (<em>uniform</em>, <em>triangular</em>, <em>epanechnikov</em>, <em>biweight</em>, <em>triweight</em>, <em>tricube</em>, <em>gaussian</em>, <em>cosine</em>, <em>logistic</em>, <em>silverman</em>, <em>inverse</em>, <em>gaussianSimple</em>, <em>exponential</em>). Here, I use the <em>canberra</em> metric and the <em>tricube</em> kernel because they give optimal results (according to my RandomSearchR package),</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">preds_TEST_tric =<span class="st"> </span><span class="kw">KernelKnn</span>(X[spl_train, ], <span class="dt">TEST_data =</span> X[spl_test, ], y[spl_train], <span class="dt">k =</span> <span class="dv">10</span> , 
                            
                            <span class="dt">method =</span> <span class="st">'canberra'</span>, <span class="dt">weights_function =</span> <span class="st">'tricube'</span>, <span class="dt">regression =</span> F,  
                            
                            <span class="dt">Levels =</span> <span class="kw">unique</span>(y))
<span class="kw">head</span>(preds_TEST_tric)</code></pre></div>
<pre><code>##           [,1]       [,2]
## [1,] 0.0000000 1.00000000
## [2,] 0.7857101 0.21428995
## [3,] 0.0000000 1.00000000
## [4,] 0.8350618 0.16493815
## [5,] 0.1763196 0.82368041
## [6,] 0.9538039 0.04619605</code></pre>
<p><br> The <strong>second option</strong> is to give a self defined kernel function. Here, I’ll pick the density function of the normal distribution with mean = 0.0 and standard deviation = 1.0 (the data are scaled to have mean zero and unit variance),</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">norm_kernel =<span class="st"> </span>function(W) {
  
  W =<span class="st"> </span><span class="kw">dnorm</span>(W, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="fl">1.0</span>)
  
  W =<span class="st"> </span>W /<span class="st"> </span><span class="kw">rowSums</span>(W)
  
  <span class="kw">return</span>(W)
}


preds_TEST_norm =<span class="st"> </span><span class="kw">KernelKnn</span>(X[spl_train, ], <span class="dt">TEST_data =</span> X[spl_test, ], y[spl_train], <span class="dt">k =</span> <span class="dv">10</span> , 
                            
                            <span class="dt">method =</span> <span class="st">'canberra'</span>, <span class="dt">weights_function =</span> norm_kernel, <span class="dt">regression =</span> F, 
                            
                            <span class="dt">Levels =</span> <span class="kw">unique</span>(y))
<span class="kw">head</span>(preds_TEST_norm)</code></pre></div>
<pre><code>##           [,1]      [,2]
## [1,] 0.0000000 1.0000000
## [2,] 0.8765587 0.1234413
## [3,] 0.0000000 1.0000000
## [4,] 0.8821567 0.1178433
## [5,] 0.1159831 0.8840169
## [6,] 0.8146508 0.1853492</code></pre>
<p><br></p>
<p>The computations can be speed up by using the parameter <strong>threads</strong> (multiple cores can be run in parallel). There is also the option to exclude <strong>extrema</strong> (minimum and maximum distances) during the calculation of the k-nearest-neighbor distances using extrema = TRUE. The <em>bandwidth</em> of the existing kernels can be tuned using the <strong>h</strong> parameter. <br></p>
<p>K-nearest-neigbor calculations in the KernelKnn function can be accomplished using the following distance metrics : <em>euclidean</em>, <em>manhattan</em>, <em>chebyshev</em>, <em>canberra</em>, <em>braycurtis</em>, <em>minkowski</em> (by default the order ‘p’ of the minkowski parameter equals k), <em>hamming</em>, <em>mahalanobis</em>, <em>pearson_correlation</em>, <em>simple_matching_coefficient</em>, <em>jaccard_coefficient</em> and <em>Rao_coefficient</em>. The last four are similarity measures and are appropriate for binary data [0,1]. <br></p>
<p>I employed my RandomSearchR package to find the optimal parameters for the KernelKnn function and the following two pairs of parameters give an optimal accuracy, <br><br></p>
<table>
<thead>
<tr class="header">
<th align="right">k</th>
<th align="left">method</th>
<th align="left">kernel</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">10</td>
<td align="left">canberra</td>
<td align="left">tricube</td>
</tr>
<tr class="even">
<td align="right">9</td>
<td align="left">canberra</td>
<td align="left">epanechnikov</td>
</tr>
</tbody>
</table>
</div>
<div id="the-kernelknncv-function" class="section level2">
<h2>The KernelKnnCV function</h2>
<p>I’ll use the <em>KernelKnnCV</em> function to calculate the accuracy using 5-fold cross-validation for the previous mentioned parameter pairs,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_cv_pair1 =<span class="st"> </span><span class="kw">KernelKnnCV</span>(X, y, <span class="dt">k =</span> <span class="dv">10</span> , <span class="dt">folds =</span> <span class="dv">5</span>, <span class="dt">method =</span> <span class="st">'canberra'</span>, 
                           
                           <span class="dt">weights_function =</span> <span class="st">'tricube'</span>, <span class="dt">regression =</span> F, 
                           
                           <span class="dt">Levels =</span> <span class="kw">unique</span>(y), <span class="dt">threads =</span> <span class="dv">5</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(fit_cv_pair1)</code></pre></div>
<pre><code>## List of 2
##  $ preds:List of 5
##   ..$ : num [1:71, 1:2] 0.00648 0.25323 1 0.97341 0.92031 ...
##   ..$ : num [1:70, 1:2] 0 0 0 0 0.999 ...
##   ..$ : num [1:70, 1:2] 0.353 0 0.17 0.212 0.266 ...
##   ..$ : num [1:70, 1:2] 0 0 0 0 0 ...
##   ..$ : num [1:70, 1:2] 0.989 0 1 0 0 ...
##  $ folds:List of 5
##   ..$ fold_1: int [1:71] 5 26 233 243 30 41 237 229 19 11 ...
##   ..$ fold_2: int [1:70] 262 89 257 67 58 266 253 85 275 268 ...
##   ..$ fold_3: int [1:70] 127 128 295 287 134 288 130 277 125 101 ...
##   ..$ fold_4: int [1:70] 313 301 317 318 316 142 175 157 146 147 ...
##   ..$ fold_5: int [1:70] 195 326 225 332 342 347 206 219 218 214 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_cv_pair2 =<span class="st"> </span><span class="kw">KernelKnnCV</span>(X, y, <span class="dt">k =</span> <span class="dv">9</span> , <span class="dt">folds =</span> <span class="dv">5</span>,<span class="dt">method =</span> <span class="st">'canberra'</span>,
                           
                           <span class="dt">weights_function =</span> <span class="st">'epanechnikov'</span>, <span class="dt">regression =</span> F,
                           
                           <span class="dt">Levels =</span> <span class="kw">unique</span>(y), <span class="dt">threads =</span> <span class="dv">5</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(fit_cv_pair2)</code></pre></div>
<pre><code>## List of 2
##  $ preds:List of 5
##   ..$ : num [1:71, 1:2] 0.0224 0.255 1 0.9601 0.8876 ...
##   ..$ : num [1:70, 1:2] 0 0 0 0 0.998 ...
##   ..$ : num [1:70, 1:2] 0.36 0 0.164 0.185 0.202 ...
##   ..$ : num [1:70, 1:2] 0 0 0 0 0 ...
##   ..$ : num [1:70, 1:2] 0.912 0 1 0 0 ...
##  $ folds:List of 5
##   ..$ fold_1: int [1:71] 5 26 233 243 30 41 237 229 19 11 ...
##   ..$ fold_2: int [1:70] 262 89 257 67 58 266 253 85 275 268 ...
##   ..$ fold_3: int [1:70] 127 128 295 287 134 288 130 277 125 101 ...
##   ..$ fold_4: int [1:70] 313 301 317 318 316 142 175 157 146 147 ...
##   ..$ fold_5: int [1:70] 195 326 225 332 342 347 206 219 218 214 ...</code></pre>
<p><br></p>
<p>Each cross-validated object returns a list of length 2 ( the first sublist includes the predictions for each fold whereas the second gives the indices of the folds)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">acc_pair1 =<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span>:<span class="kw">length</span>(fit_cv_pair1$preds), 
                          
                          function(x) <span class="kw">acc</span>(y[fit_cv_pair1$folds[[x]]], 
                                          
                                          fit_cv_pair1$preds[[x]])))
acc_pair1</code></pre></div>
<pre><code>## [1] 0.9154930 0.9142857 0.9142857 0.9285714 0.9571429</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cat</span>(<span class="st">'accurcay for params_pair1 is :'</span>, <span class="kw">mean</span>(acc_pair1), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</code></pre></div>
<pre><code>## accurcay for params_pair1 is : 0.9259557</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">acc_pair2 =<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span>:<span class="kw">length</span>(fit_cv_pair2$preds), 
                          
                          function(x) <span class="kw">acc</span>(y[fit_cv_pair2$folds[[x]]], 
                                          
                                          fit_cv_pair2$preds[[x]])))
acc_pair2</code></pre></div>
<pre><code>## [1] 0.9014085 0.9142857 0.9000000 0.9142857 0.9571429</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cat</span>(<span class="st">'accuracy for params_pair2 is :'</span>, <span class="kw">mean</span>(acc_pair2), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</code></pre></div>
<pre><code>## accuracy for params_pair2 is : 0.9174245</code></pre>
<p><br></p>
</div>
<div id="adding-or-multiplying-kernels" class="section level2">
<h2>Adding or multiplying kernels</h2>
<p>In the KernelKnn package there is also the option to <strong>combine kernels</strong> (adding or multiplying) from the existing ones. For instance, if I want to multiply the <em>tricube</em> with the <em>gaussian</em> kernel, then I’ll give the following character string to the weights_function, <em>“tricube_gaussian_MULT”</em>. On the other hand, If I want to add the same kernels then the weights_function will be <em>“tricube_gaussian_ADD”</em>. I experimented with my RandomSearchR package combining the different kernels and the following two parameter settings gave optimal results,</p>
<p><br></p>
<table>
<thead>
<tr class="header">
<th align="right">k</th>
<th align="left">method</th>
<th align="left">kernel</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">16</td>
<td align="left">canberra</td>
<td align="left">biweight_triweight_gaussian_MULT</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="left">canberra</td>
<td align="left">triangular_triweight_MULT</td>
</tr>
</tbody>
</table>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_cv_pair1 =<span class="st"> </span><span class="kw">KernelKnnCV</span>(X, y, <span class="dt">k =</span> <span class="dv">16</span>, <span class="dt">folds =</span> <span class="dv">5</span>, <span class="dt">method =</span> <span class="st">'canberra'</span>, 
                           
                           <span class="dt">weights_function =</span> <span class="st">'biweight_triweight_gaussian_MULT'</span>, 
                           
                           <span class="dt">regression =</span> F, <span class="dt">Levels =</span> <span class="kw">unique</span>(y), <span class="dt">threads =</span> <span class="dv">5</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(fit_cv_pair1)</code></pre></div>
<pre><code>## List of 2
##  $ preds:List of 5
##   ..$ : num [1:71, 1:2] 0.0015 0.1516 1 0.9763 0.9674 ...
##   ..$ : num [1:70, 1:2] 0 0 0 0 0.999 ...
##   ..$ : num [1:70, 1:2] 0.249 0 0.113 0.252 0.27 ...
##   ..$ : num [1:70, 1:2] 0 0 0 0 0 ...
##   ..$ : num [1:70, 1:2] 0.991 0 1 0 0 ...
##  $ folds:List of 5
##   ..$ fold_1: int [1:71] 5 26 233 243 30 41 237 229 19 11 ...
##   ..$ fold_2: int [1:70] 262 89 257 67 58 266 253 85 275 268 ...
##   ..$ fold_3: int [1:70] 127 128 295 287 134 288 130 277 125 101 ...
##   ..$ fold_4: int [1:70] 313 301 317 318 316 142 175 157 146 147 ...
##   ..$ fold_5: int [1:70] 195 326 225 332 342 347 206 219 218 214 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_cv_pair2 =<span class="st"> </span><span class="kw">KernelKnnCV</span>(X, y, <span class="dt">k =</span> <span class="dv">5</span>, <span class="dt">folds =</span> <span class="dv">5</span>, <span class="dt">method =</span> <span class="st">'canberra'</span>, 
                           
                           <span class="dt">weights_function =</span> <span class="st">'triangular_triweight_MULT'</span>, 
                           
                           <span class="dt">regression =</span> F, <span class="dt">Levels =</span> <span class="kw">unique</span>(y), <span class="dt">threads =</span> <span class="dv">5</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(fit_cv_pair2)</code></pre></div>
<pre><code>## List of 2
##  $ preds:List of 5
##   ..$ : num [1:71, 1:2] 0 0.0273 1 1 1 ...
##   ..$ : num [1:70, 1:2] 0 0 0 0 1 ...
##   ..$ : num [1:70, 1:2] 0.1161 0 0.0105 0.307 0.022 ...
##   ..$ : num [1:70, 1:2] 0 0 0 0 0 ...
##   ..$ : num [1:70, 1:2] 1 0 1 0 0 ...
##  $ folds:List of 5
##   ..$ fold_1: int [1:71] 5 26 233 243 30 41 237 229 19 11 ...
##   ..$ fold_2: int [1:70] 262 89 257 67 58 266 253 85 275 268 ...
##   ..$ fold_3: int [1:70] 127 128 295 287 134 288 130 277 125 101 ...
##   ..$ fold_4: int [1:70] 313 301 317 318 316 142 175 157 146 147 ...
##   ..$ fold_5: int [1:70] 195 326 225 332 342 347 206 219 218 214 ...</code></pre>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">acc_pair1 =<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span>:<span class="kw">length</span>(fit_cv_pair1$preds), 
                          
                          function(x) <span class="kw">acc</span>(y[fit_cv_pair1$folds[[x]]], 
                                          
                                          fit_cv_pair1$preds[[x]])))
acc_pair1</code></pre></div>
<pre><code>## [1] 0.9014085 0.9142857 0.9285714 0.9285714 0.9571429</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cat</span>(<span class="st">'accuracy for params_pair1 is :'</span>, <span class="kw">mean</span>(acc_pair1), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</code></pre></div>
<pre><code>## accuracy for params_pair1 is : 0.925996</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">acc_pair2 =<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span>:<span class="kw">length</span>(fit_cv_pair2$preds), 
                          
                          function(x) <span class="kw">acc</span>(y[fit_cv_pair2$folds[[x]]],
                                          
                                          fit_cv_pair2$preds[[x]])))
acc_pair2</code></pre></div>
<pre><code>## [1] 0.9014085 0.9285714 0.9285714 0.9142857 0.9714286</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cat</span>(<span class="st">'accuracy for params_pair2 is :'</span>, <span class="kw">mean</span>(acc_pair2), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</code></pre></div>
<pre><code>## accuracy for params_pair2 is : 0.9288531</code></pre>
<p><br></p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
