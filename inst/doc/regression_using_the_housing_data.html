<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Lampros Mouselimis" />

<meta name="date" content="2016-12-07" />

<title>Regression using the Housing data</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Regression using the Housing data</h1>
<h4 class="author"><em>Lampros Mouselimis</em></h4>
<h4 class="date"><em>2016-12-07</em></h4>



<p>The following examples illustrate the functionality of the KernelKnn package for <strong>regression</strong> tasks. I’ll make use of the <em>Housing</em> data set,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(Boston, <span class="dt">package =</span> <span class="st">'KernelKnn'</span>)

<span class="kw">str</span>(Boston)</code></pre></div>
<pre><code>## 'data.frame':    506 obs. of  14 variables:
##  $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...
##  $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...
##  $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...
##  $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...
##  $ rm     : num  6.58 6.42 7.18 7 7.15 ...
##  $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...
##  $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...
##  $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...
##  $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...
##  $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...
##  $ black  : num  397 397 393 395 397 ...
##  $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...
##  $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...</code></pre>
<p><br></p>
<p>When using an algorithm where the ouput depends on distance calculation (as is the case in k-nearest-neighbors) it is recommended to first scale the data,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X =<span class="st"> </span><span class="kw">scale</span>(Boston[, -<span class="kw">ncol</span>(Boston)])
y =<span class="st"> </span>Boston[, <span class="kw">ncol</span>(Boston)]

<span class="co"># random split of data in train and test</span>

spl_train =<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">length</span>(y), <span class="kw">round</span>(<span class="kw">length</span>(y) *<span class="st"> </span><span class="fl">0.75</span>))
spl_test =<span class="st"> </span><span class="kw">setdiff</span>(<span class="dv">1</span>:<span class="kw">length</span>(y), spl_train)
<span class="kw">str</span>(spl_train)</code></pre></div>
<pre><code>##  int [1:380] 9 57 407 476 41 305 281 430 177 380 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(spl_test)</code></pre></div>
<pre><code>##  int [1:126] 2 3 12 13 14 23 27 28 36 38 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># evaluation metric</span>

mse =<span class="st"> </span>function (y_true, y_pred) {
  
  out =<span class="st"> </span><span class="kw">mean</span>((y_true -<span class="st"> </span>y_pred)^<span class="dv">2</span>)
  
  out
}</code></pre></div>
<div id="the-kernelknn-function" class="section level2">
<h2>The KernelKnn function</h2>
<p>The KernelKnn function takes a number of arguments. To read details for each one of the arguments type ?KernelKnn::KernelKnn in the console.</p>
<p>A simple k-nearest-neighbors can be run with weights_function = NULL (the parameter ‘regression’ should be set to TRUE for regression),</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(KernelKnn)

preds_TEST =<span class="st"> </span><span class="kw">KernelKnn</span>(X[spl_train, ], <span class="dt">TEST_data =</span> X[spl_test, ], y[spl_train], <span class="dt">k =</span> <span class="dv">5</span> , 
                       
                       <span class="dt">method =</span> <span class="st">'euclidean'</span>, <span class="dt">weights_function =</span> <span class="ot">NULL</span>, <span class="dt">regression =</span> T)
<span class="kw">str</span>(preds_TEST)</code></pre></div>
<pre><code>##  num [1:126] 23.3 31.3 20.8 21.5 19.6 ...</code></pre>
<p><br> Using transf_categ_cols = TRUE, categorical features can be either encoded to dummy or to numeric features depending on the number of the unique values (here I convert the ‘chas’ and ‘rad’ features to factor to apply the <em>transf_categ_cols</em> parameter)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">apply</span>(Boston, <span class="dv">2</span>, function(x) <span class="kw">length</span>(<span class="kw">unique</span>(x)))</code></pre></div>
<pre><code>##    crim      zn   indus    chas     nox      rm     age     dis     rad 
##     504      26      76       2      81     446     356     412       9 
##     tax ptratio   black   lstat    medv 
##      66      46     357     455     229</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tmp_bst =<span class="st"> </span>Boston
tmp_bst$chas =<span class="st"> </span><span class="kw">as.factor</span>(tmp_bst$chas)
tmp_bst$rad =<span class="st"> </span><span class="kw">as.factor</span>(tmp_bst$rad)

preds_TEST =<span class="st"> </span><span class="kw">KernelKnn</span>(tmp_bst[spl_train, -<span class="kw">ncol</span>(tmp_bst)], 
                       
                       <span class="dt">TEST_data =</span> tmp_bst[spl_test, -<span class="kw">ncol</span>(tmp_bst)], 
                       
                       y[spl_train], <span class="dt">k =</span> <span class="dv">5</span> , <span class="dt">method =</span> <span class="st">'euclidean'</span>, 
                       
                       <span class="dt">regression =</span> T, <span class="dt">transf_categ_cols =</span> T)
<span class="kw">str</span>(preds_TEST)</code></pre></div>
<pre><code>##  num [1:126] 20.7 22.9 20.2 24 21 ...</code></pre>
<p><br> There are two ways to use a kernel in the KernelKnn function. The <strong>first option</strong> is to choose one of the existing kernels (<em>uniform</em>, <em>triangular</em>, <em>epanechnikov</em>, <em>biweight</em>, <em>triweight</em>, <em>tricube</em>, <em>gaussian</em>, <em>cosine</em>, <em>logistic</em>, <em>silverman</em>, <em>inverse</em>, <em>gaussianSimple</em>, <em>exponential</em>). Here, I use the <em>mahalanobis</em> metric (which takes advantage of the covariance matrix of the data, but it somewhat slows down training in comparison to the other distance metrics) and the <em>biweight</em> kernel, because they give optimal results (according to my <em>RandomSearchR</em> package),</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">preds_TEST_biw =<span class="st"> </span><span class="kw">KernelKnn</span>(X[spl_train, ], <span class="dt">TEST_data =</span> X[spl_test, ], y[spl_train], <span class="dt">k =</span> <span class="dv">5</span>, 
                           
                           <span class="dt">method =</span> <span class="st">'mahalanobis'</span>, <span class="dt">weights_function =</span> <span class="st">'biweight'</span>, 
                           
                           <span class="dt">regression =</span> T, <span class="dt">transf_categ_cols =</span> F)
<span class="kw">str</span>(preds_TEST_biw)</code></pre></div>
<pre><code>##  num [1:126] 22.7 35.1 22.1 20.8 19.3 ...</code></pre>
<p><br> The <strong>second option</strong> is to give a self defined kernel function. Here, I’ll pick the density function of the normal distribution with mean = 0.0 and standard deviation = 1.0 (the data are scaled to have mean zero and unit variance),</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">norm_kernel =<span class="st"> </span>function(W) {
  
  W =<span class="st"> </span><span class="kw">dnorm</span>(W, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="fl">1.0</span>)
  
  W =<span class="st"> </span>W /<span class="st"> </span><span class="kw">rowSums</span>(W)
  
  <span class="kw">return</span>(W)
}


preds_TEST_norm =<span class="st"> </span><span class="kw">KernelKnn</span>(X[spl_train, ], <span class="dt">TEST_data =</span> X[spl_test, ], y[spl_train], <span class="dt">k =</span> <span class="dv">5</span>,
                            
                            <span class="dt">method =</span> <span class="st">'mahalanobis'</span>, <span class="dt">weights_function =</span> norm_kernel, 
                            
                            <span class="dt">regression =</span> T, <span class="dt">transf_categ_cols =</span> F)
<span class="kw">str</span>(preds_TEST_norm)</code></pre></div>
<pre><code>##  num [1:126] 23.6 31.1 22.3 22.1 20.3 ...</code></pre>
<p><br></p>
<p>The computations can be speed up by using the parameter <strong>threads</strong> (multiple cores can be run in parallel). There is also the option to exclude <strong>extrema</strong> (minimum and maximum distances) during the calculation of the k-nearest-neighbor distances using extrema = TRUE. The <em>bandwidth</em> of the existing kernels can be tuned using the <strong>h</strong> parameter. <br></p>
<p>K-nearest-neigbor calculations in the KernelKnn function can be accomplished using the following distance metrics : <em>euclidean</em>, <em>manhattan</em>, <em>chebyshev</em>, <em>canberra</em>, <em>braycurtis</em>, <em>minkowski</em> (by default the order ‘p’ of the minkowski parameter equals k), <em>hamming</em>, <em>mahalanobis</em>, <em>pearson_correlation</em>, <em>simple_matching_coefficient</em>, <em>jaccard_coefficient</em> and <em>Rao_coefficient</em>. The last four are similarity measures and are appropriate for binary data [0,1]. <br></p>
<p>I employed my RandomSearchR package to find the optimal parameters for the KernelKnn function and the following two pairs of parameters give an optimal mean-squared-error, <br> <br></p>
<table>
<thead>
<tr class="header">
<th align="right">k</th>
<th align="left">method</th>
<th align="left">kernel</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">9</td>
<td align="left">mahalanobis</td>
<td align="left">triweight</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="left">canberra</td>
<td align="left">cosine</td>
</tr>
</tbody>
</table>
</div>
<div id="the-kernelknncv-function" class="section level2">
<h2>The KernelKnnCV function</h2>
<p>I’ll use the <em>KernelKnnCV</em> function to calculate the mean-squared-error using 3-fold cross-validation for the previous mentioned parameter pairs,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_cv_pair1 =<span class="st"> </span><span class="kw">KernelKnnCV</span>(X, y, <span class="dt">k =</span> <span class="dv">9</span>, <span class="dt">folds =</span> <span class="dv">3</span>, <span class="dt">method =</span> <span class="st">'mahalanobis'</span>, 
                           
                           <span class="dt">weights_function =</span> <span class="st">'triweight'</span>, <span class="dt">regression =</span> T, <span class="dt">threads =</span> <span class="dv">5</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(fit_cv_pair1)</code></pre></div>
<pre><code>## List of 2
##  $ preds:List of 3
##   ..$ : num [1:168] 24.1 32.4 17.6 16.6 19.4 ...
##   ..$ : num [1:169] 30 21.8 21.5 20.5 22.6 ...
##   ..$ : num [1:169] 23.3 29.7 30.8 20.4 22.6 ...
##  $ folds:List of 3
##   ..$ fold_1: int [1:168] 2 5 8 9 14 23 24 29 34 35 ...
##   ..$ fold_2: int [1:169] 6 12 13 16 17 19 20 22 30 36 ...
##   ..$ fold_3: int [1:169] 1 3 4 7 10 11 15 18 21 25 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_cv_pair2 =<span class="st"> </span><span class="kw">KernelKnnCV</span>(X, y, <span class="dt">k =</span> <span class="dv">3</span>, <span class="dt">folds =</span> <span class="dv">3</span>, <span class="dt">method =</span> <span class="st">'canberra'</span>,
                           
                           <span class="dt">weights_function =</span> <span class="st">'cosine'</span>, <span class="dt">regression =</span> T, <span class="dt">threads =</span> <span class="dv">5</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(fit_cv_pair2)</code></pre></div>
<p><br></p>
<p>Each cross-validated object returns a list of length 2 ( the first sublist includes the predictions for each fold whereas the second gives the indices of the folds)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mse_pair1 =<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span>:<span class="kw">length</span>(fit_cv_pair1$preds), 
                          
                          function(x) <span class="kw">mse</span>(y[fit_cv_pair1$folds[[x]]], 
                                          
                                          fit_cv_pair1$preds[[x]])))
mse_pair1</code></pre></div>
<pre><code>## [1] 18.17392 18.29160 11.01078</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cat</span>(<span class="st">'mse for params_pair1 is :'</span>, <span class="kw">mean</span>(mse_pair1), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</code></pre></div>
<pre><code>## mse for params_pair1 is : 15.82543</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mse_pair2 =<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span>:<span class="kw">length</span>(fit_cv_pair2$preds), 
                          
                          function(x) <span class="kw">mse</span>(y[fit_cv_pair2$folds[[x]]], 
                                          
                                          fit_cv_pair2$preds[[x]])))
mse_pair2</code></pre></div>
<pre><code>## [1] 26.84027 22.75759 18.53496</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cat</span>(<span class="st">'mse for params_pair2 is :'</span>, <span class="kw">mean</span>(mse_pair2), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</code></pre></div>
<pre><code>## mse for params_pair2 is : 22.71094</code></pre>
<p><br></p>
</div>
<div id="adding-or-multiplying-kernels" class="section level2">
<h2>Adding or multiplying kernels</h2>
<p>In the KernelKnn package there is also the option to <strong>combine kernels</strong> (adding or multiplying) from the existing ones. For instance, if I want to multiply the <em>tricube</em> with the <em>gaussian</em> kernel, then I’ll give the following character string to the weights_function, <em>“tricube_gaussian_MULT”</em>. On the other hand, If I want to add the same kernels then the weights_function will be <em>“tricube_gaussian_ADD”</em>. I experimented with my RandomSearchR package combining the different kernels and the following two parameter settings gave optimal results,</p>
<p><br></p>
<table>
<thead>
<tr class="header">
<th align="right">k</th>
<th align="left">method</th>
<th align="left">kernel</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">19</td>
<td align="left">mahalanobis</td>
<td align="left">triangular_triweight_MULT</td>
</tr>
<tr class="even">
<td align="right">18</td>
<td align="left">mahalanobis</td>
<td align="left">biweight_triweight_gaussian_MULT</td>
</tr>
</tbody>
</table>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_cv_pair1 =<span class="st"> </span><span class="kw">KernelKnnCV</span>(X, y, <span class="dt">k =</span> <span class="dv">19</span>, <span class="dt">folds =</span> <span class="dv">3</span>, <span class="dt">method =</span> <span class="st">'mahalanobis'</span>, 
                           
                           <span class="dt">weights_function =</span> <span class="st">'triangular_triweight_MULT'</span>, 
                           
                           <span class="dt">regression =</span> T, <span class="dt">threads =</span> <span class="dv">5</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(fit_cv_pair1)</code></pre></div>
<pre><code>## List of 2
##  $ preds:List of 3
##   ..$ : num [1:168] 24.3 32.4 17.8 17.1 19.5 ...
##   ..$ : num [1:169] 28.6 21.5 21.9 20.4 22.2 ...
##   ..$ : num [1:169] 23.2 27.6 31.1 20.7 22.4 ...
##  $ folds:List of 3
##   ..$ fold_1: int [1:168] 2 5 8 9 14 23 24 29 34 35 ...
##   ..$ fold_2: int [1:169] 6 12 13 16 17 19 20 22 30 36 ...
##   ..$ fold_3: int [1:169] 1 3 4 7 10 11 15 18 21 25 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_cv_pair2 =<span class="st"> </span><span class="kw">KernelKnnCV</span>(X, y, <span class="dt">k =</span> <span class="dv">18</span>, <span class="dt">folds =</span> <span class="dv">3</span>, <span class="dt">method =</span> <span class="st">'mahalanobis'</span>, 
                           
                           <span class="dt">weights_function =</span> <span class="st">'biweight_triweight_gaussian_MULT'</span>, 
                           
                           <span class="dt">regression =</span> T, <span class="dt">threads =</span> <span class="dv">5</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(fit_cv_pair2)</code></pre></div>
<pre><code>## List of 2
##  $ preds:List of 3
##   ..$ : num [1:168] 24.2 32.5 17.8 17.1 19.4 ...
##   ..$ : num [1:169] 28.9 21.7 21.8 20.4 22.1 ...
##   ..$ : num [1:169] 23.3 27.7 31.2 20.6 22.4 ...
##  $ folds:List of 3
##   ..$ fold_1: int [1:168] 2 5 8 9 14 23 24 29 34 35 ...
##   ..$ fold_2: int [1:169] 6 12 13 16 17 19 20 22 30 36 ...
##   ..$ fold_3: int [1:169] 1 3 4 7 10 11 15 18 21 25 ...</code></pre>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mse_pair1 =<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span>:<span class="kw">length</span>(fit_cv_pair1$preds), 
                          
                          function(x) <span class="kw">mse</span>(y[fit_cv_pair1$folds[[x]]], 
                                          
                                          fit_cv_pair1$preds[[x]])))
mse_pair1</code></pre></div>
<pre><code>## [1] 18.73885 18.64990 11.21641</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cat</span>(<span class="st">'mse for params_pair1 is :'</span>, <span class="kw">mean</span>(mse_pair1), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</code></pre></div>
<pre><code>## mse for params_pair1 is : 16.20172</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mse_pair2 =<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span>:<span class="kw">length</span>(fit_cv_pair2$preds), 
                          
                          function(x) <span class="kw">mse</span>(y[fit_cv_pair2$folds[[x]]], 
                                          
                                          fit_cv_pair2$preds[[x]])))
mse_pair2</code></pre></div>
<pre><code>## [1] 19.12602 18.63044 11.21680</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cat</span>(<span class="st">'mse for params_pair2 is :'</span>, <span class="kw">mean</span>(mse_pair2), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</code></pre></div>
<pre><code>## mse for params_pair2 is : 16.32442</code></pre>
<p><br></p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
